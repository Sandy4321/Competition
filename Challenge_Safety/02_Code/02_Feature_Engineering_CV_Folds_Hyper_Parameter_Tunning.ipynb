{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For reading data filenames \n",
    "import glob\n",
    "\n",
    "\n",
    "# For calculating Time \n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# For processing data\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For Features Creation\n",
    "from feature_engineering import *\n",
    "\n",
    "## For Model Building\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "## For Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, accuracy_score\n",
    "\n",
    "## For Plotting Graphs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "## for cross validation\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "## Hyperparameter Tunning\n",
    "import hyperopt\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
    "\n",
    "#from keras import Sequential\n",
    "#from keras.layers import Dense, LSTM\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data and sorted the data at boookingID*second level\n",
    "\n",
    "def data_load_fun(train_path):\n",
    "    start = datetime.datetime.now().replace(microsecond=0)\n",
    "    features = pd.DataFrame()\n",
    "    filename = []\n",
    "    for files in glob.glob(train_path):\n",
    "        filename.append(files)\n",
    "    for file in filename:\n",
    "        print (file)\n",
    "        tmp_df = pd.read_csv(file)\n",
    "        features = pd.concat([features,tmp_df], axis=0)\n",
    "    features = features.sort_values(by= ['bookingID',\"second\"])\n",
    "    end = datetime.datetime.now().replace(microsecond=0)\n",
    "    print (len(filename), \" Files Loaded Successfuly; Time Taken -->\" ,end-start)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_processing(df):\n",
    "    df = df[df.second <10000]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features Creation\n",
    "def features_creation(features):\n",
    "    start = datetime.datetime.now().replace(microsecond=0)\n",
    "    \n",
    "    features = generic_features_creation_01(features)\n",
    "    \n",
    "    windows_feas1 = window_features_creation1_03(features,window_size=10,over_wd=5,\n",
    "                                   cols =['Accuracy','Bearing','Speed','gyro','acceleration','acc_cal'])\n",
    "    windows_feas2 = window_features_creation1_03(features,window_size=10,over_wd=5,\n",
    "                                   cols =['signal_weak','stop','trip_start','missing_ind','trip_end'], \n",
    "                                    metrics = {\"sum\",\"max\"})\n",
    "    window_feas_final = pd.merge(windows_feas1, windows_feas2, on= ['bookingID','window'], how = \"left\")\n",
    "    end = datetime.datetime.now().replace(microsecond=0)\n",
    "    print (window_feas_final.shape[1], \" Features Created at sliding waindow of 10s with overlap of 5s; Time Taken->\", end-start)\n",
    "    \n",
    "    fset1 = generic_trip_features_02(features)\n",
    "    fset2 = window_grp_stop_04(window_feas_final)\n",
    "    fset3 = window_grp_speed_05(window_feas_final)\n",
    "    fset4 = window_grp_bearing_06(window_feas_final, cols = ['Bearing_std',\"Bearing_max\",\"Bearing_min\"], thres=10, var_nm=\"turn\")\n",
    "    fset5 = trip_ending_fes_07(window_feas_final)\n",
    "    fset6 = window_grp_accuracy_08(window_feas_final)\n",
    "    fset7 = generic_stats_features_09(window_feas_final, cols= [\"Speed\",\"acc_cal\",\"Accuracy\",\"acceleration\",\"gyro\"])\n",
    "    fset8 = events_calculation_10(window_feas_final)\n",
    "\n",
    "    fset=fset1.copy()\n",
    "    fset = fset.merge(fset2, on = ['bookingID'], how = \"left\")\n",
    "    fset = fset.merge(fset3, on = ['bookingID'], how = \"left\")\n",
    "    fset = fset.merge(fset4, on = ['bookingID'], how = \"left\")\n",
    "    fset = fset.merge(fset5, on = ['bookingID'], how = \"left\")\n",
    "    fset = fset.merge(fset6, on = ['bookingID'], how = \"left\")\n",
    "    fset = fset.merge(fset7, on = ['bookingID'], how = \"left\")\n",
    "    fset = fset.merge(fset8, on = ['bookingID'], how = \"left\")\n",
    "    \n",
    "    end1 = datetime.datetime.now().replace(microsecond=0)\n",
    "    print (fset.shape[1], \" Features Created from windows data; Time taken ->\", end1- end)\n",
    "    return fset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../01_Data/Train/part-00001-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00007-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00002-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00003-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00004-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00008-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00009-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00006-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00005-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "../01_Data/Train/part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv\n",
      "10  Files Loaded Successfuly; Time Taken --> 0:00:55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd6e59e4e0aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_load_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pre_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfeatures_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-18841e0fc06f>\u001b[0m in \u001b[0;36mfeatures_creation\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      9\u001b[0m     windows_feas2 = window_features_creation1_03(features,window_size=10,over_wd=5,\n\u001b[1;32m     10\u001b[0m                                    \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal_weak'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trip_start'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'missing_ind'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'trip_end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                     metrics = {\"sum\",\"max\"})\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mwindow_feas_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows_feas1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows_feas2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bookingID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'window'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmicrosecond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/shubham.gupta/competitions/Grab/Challenge_Safety/02_Code/feature_engineering.py\u001b[0m in \u001b[0;36mwindow_features_creation1_03\u001b[0;34m(df, window_size, over_wd, cols, metrics)\u001b[0m\n\u001b[1;32m    102\u001b[0m                         cols =['Accuracy','Bearing','Speed','acceleration','gyro','acc_cal','signal_weak','stop',\n\u001b[1;32m    103\u001b[0m                                'trip_start','missing_ind','trip_end'],\n\u001b[0;32m--> 104\u001b[0;31m                        metrics ={\"mean\",\"std\",\"min\",\"max\"} ):\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2916\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2966\u001b[0m         \u001b[0;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2967\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2968\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2969\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_path = '../01_Data/Train/*.csv'\n",
    "label_path = '../01_Data/labels/part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv'\n",
    "#train_path = '../01_Data/features/*.csv'\n",
    "train_data = data_load_fun(train_path)\n",
    "train_data = data_pre_processing(train_data)\n",
    "features_all = features_creation(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read Labels and Incase duplicate value keep the positive one \n",
    "labels = pd.read_csv(label_path)\n",
    "labels = labels.groupby(['bookingID']).max().reset_index()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = pd.merge(features_all, labels, on= ['bookingID'], how=\"left\")\n",
    "\n",
    "drop_cols = ['bookingID','label']\n",
    "target = features_all[['bookingID','label']]\n",
    "features_all = features_all.drop(columns = drop_cols, axis=1)\n",
    "\n",
    "train_df, val_df, train_target, val_target = train_test_split(features_all, target, test_size=0.2, random_state=1259)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "\n",
    "best_params = {'num_leaves': 8, 'colsample_bytree': 0.7414265939570617, 'max_depth': 6, \n",
    "               'learning_rate': 0.019922131503094735, 'subsample': 0.5675422486112608,\n",
    "               'min_data_in_leaf': 51, 'min_sum_hessian_in_leaf': 10, 'bagging_freq': 9,\n",
    "               'scale_pos_weight': (1 - np.mean(train_target.label))/np.mean(train_target.label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X=train_df, X_test=val_df, y=train_target[['label']], params=None, folds=folds,\n",
    "                plot_feature_importance=False):\n",
    "    print (\"Train Data -- >\", X.shape[0])\n",
    "    print (\"Test Data -- >\", X_test.shape[0])\n",
    "        \n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        \n",
    "       \n",
    "\n",
    "        model = lgb.LGBMClassifier(**params, n_estimators = 2000, nthread = 4, n_jobs = -1)\n",
    "        model.fit(X_train, y_train, \n",
    "                        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='auc',\n",
    "                        verbose=500, early_stopping_rounds=300)\n",
    "\n",
    "        y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "        y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:,1]\n",
    "            \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        fpr, tpr, thresholds = roc_curve(y_valid.label,y_pred_valid)\n",
    "        roc_auc_valid = auc(fpr, tpr)\n",
    "        scores.append(roc_auc_valid)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "      \n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = X.columns\n",
    "        fold_importance[\"importance\"] = model.feature_importances_\n",
    "        fold_importance[\"fold\"] = fold_n + 1\n",
    "        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    \n",
    "    feature_importance[\"importance\"] /= n_fold\n",
    "    if plot_feature_importance:\n",
    "        cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "        plt.figure(figsize=(16, 12));\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "        plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "        return oof, prediction, feature_importance\n",
    "    return oof, prediction\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof, pred, fp = train_model(params=best_params, plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = oof\n",
    "val_pred = pred\n",
    "\n",
    "print (\"stats--->\")\n",
    "print (\"\")\n",
    "#y_train_pred = len*[0]\n",
    "\n",
    "\n",
    "fpr_train, tpr_train, thresholds = roc_curve(train_target.label,train_pred)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "fpr_val, tpr_val, thresholds = roc_curve(val_target.label,val_pred)\n",
    "roc_auc_val = auc(fpr_val, tpr_val)\n",
    "\n",
    "print (\"Train ROC --> \", roc_auc_train)\n",
    "print (\"Val ROC --> \", roc_auc_val)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Train ROC curve (area = %0.2f)' % roc_auc_train)\n",
    "plt.plot(fpr_val, tpr_val, color='red', lw=2, label='Val ROC curve (area = %0.2f)' % roc_auc_val)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParamter Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_dict={}\n",
    "space = {'max_depth': hp.choice('max_depth', np.arange(-1, 10,dtype=int)),\n",
    "         'min_data_in_leaf': hp.choice('min_data_in_leaf', np.arange(10, 400,dtype=int)),\n",
    "         'min_sum_hessian_in_leaf': hp.choice('min_sum_hessian_in_leaf', np.arange(0, 15,dtype=int)),\n",
    "         'num_leaves': hp.choice('num_leaves', np.arange(2, 20, dtype=int)),\n",
    "         'bagging_freq': hp.choice('bagging_freq', np.arange(1, 20, dtype=int)),\n",
    "         'subsample': hp.uniform('subsample', 0, 1),\n",
    "         'colsample_bytree': hp.uniform('colsample_bytree', 0, 1),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "        }\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    params = {\n",
    "        'min_sum_hessian_in_leaf': space['min_sum_hessian_in_leaf'],\n",
    "        'min_data_in_leaf':space['min_data_in_leaf'],\n",
    "            'num_leaves':space['num_leaves'],\n",
    "             'subsample': space['subsample'],\n",
    "            'colsample_bytree': space['colsample_bytree'],\n",
    "            'learning_rate':space['learning_rate'],\n",
    "            'silent': 1,\n",
    "            'verbose_eval': True,\n",
    "            \"objective\":\"binary\",\n",
    "        'device':'cpu',\n",
    "        \"boosting\":\"gbdt\",\n",
    "        'max_depth':space['max_depth'],\n",
    "        'bagging_freq':space['bagging_freq'],\n",
    "          'metric':\"auc\",\n",
    "    'boost_from_average':False,\n",
    "    }\n",
    "    lgtrain = lgb.Dataset(features_all, label=target.label.values)\n",
    "    cv = lgb.cv(params,\n",
    "                lgtrain,\n",
    "                nfold=5,metrics='auc',\n",
    "                num_boost_round=20000,\n",
    "                early_stopping_rounds=600,stratified=True,shuffle=True,verbose_eval=-1)\n",
    "    au = (cv['auc-mean'][-1])\n",
    "    params['n_estimators']=len(cv['auc-mean'])\n",
    "    param_dict[au]=params\n",
    "    pickle.dump(param_dict,open('params','wb'))\n",
    "    #print(params, file=open(\"output_lgb.txt\", \"a\"))\n",
    "    #print(params)\n",
    "    #print('max= ',round(max(param_dict.keys()),5) )\n",
    "    #print ('auc= ', round(au,3))\n",
    "    #print(au, file=open(\"output_lgb.txt\", \"a\"))\n",
    "    return{'loss': -au, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = hyperopt.fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=1000,\n",
    "            trials=trials\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_freq': 9,\n",
       " 'colsample_bytree': 0.7414265939570617,\n",
       " 'learning_rate': 0.019922131503094735,\n",
       " 'max_depth': 6,\n",
       " 'min_data_in_leaf': 51,\n",
       " 'min_sum_hessian_in_leaf': 10,\n",
       " 'num_leaves': 8,\n",
       " 'subsample': 0.5675422486112608}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
